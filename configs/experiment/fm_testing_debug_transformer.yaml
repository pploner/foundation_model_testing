# @package _global_

# to execute this experiment run:
# python train.py experiment=fm_testing

defaults:
  - override /data: collide2v
  - override /callbacks: default
  - override /trainer: gpu

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["collide2v", "tinyTransformer", "ReduceLROnPlateau", "optimizer_sweep"]

data:
  train_val_test_split_per_class: [1_000_000, 20_000, 20_000]
  label: "QCD_ggHbb_upscaled"
  to_classify:
    QCD: "QCD_inclusive"
    ggHbb: "ggHbb"

trainer:
  min_epochs: 1
  max_epochs: 1000

callbacks:
  early_stopping:
    monitor: "val/acc"
    min_delta: 0.001
    patience: 10

model:
  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    patience: 2
    factor: 0.5
    min_lr: 1e-06

preprocess:
  enabled: true

logger:
  mlflow:
    run_name: "Optimizer_sweep"
