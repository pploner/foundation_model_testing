# @package _global_

# to execute this experiment run:
# python train.py experiment=fm_testing

defaults:
  - override /data: collide2v_emptyDatasetConfig
  - override /callbacks: default
  - override /trainer: gpu

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["collide2v", "tinyTransformer", "version2_onlyMisc"]

data:
  train_val_test_split_per_class: [4_960_000, 20_000, 20_000]
  label: "v2_QCD_ggHbb_upscaled_onlyMisc"
  to_classify:
    QCD: "QCD_inclusive"
    ggHbb: "ggHbb"
  datasets_config:
    electrons:
      cols:
        - FullReco_Electron_PT
        - FullReco_Electron_Eta
        - FullReco_Electron_Phi
        - FullReco_Electron_EhadOverEem
        - FullReco_Electron_IsolationVarRhoCorr
      topk: 1
      count: false

    muons:
      cols:
        - FullReco_MuonTight_PT
        - FullReco_MuonTight_Eta
        - FullReco_MuonTight_Phi
        - FullReco_MuonTight_IsolationVarRhoCorr
      topk: 1
      count: false

    photons:
      cols:
        - FullReco_PhotonTight_PT
        - FullReco_PhotonTight_Eta
        - FullReco_PhotonTight_Phi
      topk: 1
      count: false

    puppi_met:
      cols:
        - FullReco_PUPPIMET_MET
        - FullReco_PUPPIMET_Phi
      topk: null   # scalar values
      count: false

trainer:
  min_epochs: 1
  max_epochs: 1000

callbacks:
  early_stopping:
    monitor: "val/acc"
    min_delta: 0.0001
    patience: 10

model:
  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    patience: 2
    factor: 0.5
    min_lr: 1e-06

preprocess:
  enabled: true

logger:
  mlflow:
    run_name: "Optimized_upscaled_v2_onlyMisc"
    experiment_name: "hyperparameter_search"
