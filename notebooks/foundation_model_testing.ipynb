{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5cca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "# ====== USER CONFIG ======\n",
    "EOS_BASE = \"/eos/project/f/foundational-model-dataset/samples/production_final\"\n",
    "EOS_TARGET = \"/eos/user/p/phploner/foundation_model_testing_data\"\n",
    "AFS_TMPDIR = \"/eos/user/p/phploner/foundation_model_testing_data/tmpdir\"\n",
    "\n",
    "# SELECTED_6 = {\n",
    "#    \"DY\": \"DY to ll\",\n",
    "#    \"QCD\": \"QCD inclusive\",\n",
    "#    \"SingleHiggs\": \"VBFHtautau\",\n",
    "#    \"top\": \"tt all-lept\",\n",
    "#    \"diboson\": \"WZ (semi-leptonic)\",\n",
    "#    \"diHiggs\": \"HH bbtautau\",\n",
    "# }\n",
    "\n",
    "\n",
    "# Feature packing hyperparams\n",
    "K_PART = 20  # top-K PUPPI particles by pT\n",
    "K_JET = 4  # top-J AK4 jets by pT\n",
    "\n",
    "# Training config\n",
    "EVENTS_PER_CLASS = 500000\n",
    "TRAIN_VAL_RATIO = 0.95\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "LR = 2e-3\n",
    "SEED = 42\n",
    "SAVE_LABEL = \"classifier_test\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFY HERE WHICH COLUMNS OF THE DATASET TO TAKE FOR THE TRAINING\n",
    "# The columns are given in groups (e.g. particles, jets etc.)\n",
    "# cols: are the specific columns to be read\n",
    "# topk: are the amount of objects to be taken from the columns, they are sorted in descending order w.r.t. the first col\n",
    "# e.g. \"particles\": {\n",
    "#       \"cols\": [\n",
    "#           \"FullReco_PUPPIPart_PT\",\n",
    "#           \"FullReco_PUPPIPart_Eta\",\n",
    "#           \"FullReco_PUPPIPart_Phi\",\n",
    "#       ]\n",
    "#       \"topk\": 20,\n",
    "# takes the 20 particles with the highest PT (first col) and sorts them from highest to lowest\n",
    "# topk = 1 means only the highest object w.r.t the first column is taken\n",
    "# topk = None is used for scalar values where no ordering/selecting needs to be done (e.g. MET)\n",
    "# count: is a bool that specifies if an additional column should be made with the total number of objects (e.g. # of particles)\n",
    "\n",
    "\n",
    "DATASETS_CONFIG = {\n",
    "    \"particles\": {\n",
    "        \"cols\": [\n",
    "            \"FullReco_PUPPIPart_PT\",\n",
    "            \"FullReco_PUPPIPart_Eta\",\n",
    "            \"FullReco_PUPPIPart_Phi\",\n",
    "            \"FullReco_PUPPIPart_Charge\",\n",
    "            \"FullReco_PUPPIPart_Mass\",\n",
    "            \"FullReco_PUPPIPart_PID\",\n",
    "            \"FullReco_PUPPIPart_PuppiW\",\n",
    "        ],\n",
    "        \"topk\": K_PART,\n",
    "        \"count\": True,\n",
    "    },\n",
    "    \"jets\": {\n",
    "        \"cols\": [\n",
    "            \"FullReco_JetAK4_PT\",\n",
    "            \"FullReco_JetAK4_Eta\",\n",
    "            \"FullReco_JetAK4_Phi\",\n",
    "            \"FullReco_JetAK4_Mass\",\n",
    "            \"FullReco_JetAK4_BTag\",\n",
    "            \"FullReco_JetAK4_Charge\",\n",
    "        ],\n",
    "        \"topk\": K_JET,\n",
    "        \"count\": True,\n",
    "    },\n",
    "    \"electrons\": {\n",
    "        \"cols\": [\n",
    "            \"FullReco_Electron_PT\",\n",
    "            \"FullReco_Electron_Eta\",\n",
    "            \"FullReco_Electron_Phi\",\n",
    "            \"FullReco_Electron_EhadOverEem\",\n",
    "            \"FullReco_Electron_IsolationVarRhoCorr\",\n",
    "        ],\n",
    "        \"topk\": 1,\n",
    "        \"count\": False,\n",
    "    },\n",
    "    \"muons\": {\n",
    "        \"cols\": [\n",
    "            \"FullReco_MuonTight_PT\",\n",
    "            \"FullReco_MuonTight_Eta\",\n",
    "            \"FullReco_MuonTight_Phi\",\n",
    "            \"FullReco_MuonTight_IsolationVarRhoCorr\",\n",
    "        ],\n",
    "        \"topk\": 1,\n",
    "        \"count\": False,\n",
    "    },\n",
    "    \"photons\": {\n",
    "        \"cols\": [\n",
    "            \"FullReco_PhotonTight_PT\",\n",
    "            \"FullReco_PhotonTight_Eta\",\n",
    "            \"FullReco_PhotonTight_Phi\",\n",
    "        ],\n",
    "        \"topk\": 1,\n",
    "        \"count\": False,\n",
    "    },\n",
    "    \"primary_vertex\": {\n",
    "        \"cols\": [\n",
    "            \"FullReco_PrimaryVertex_SumPT2\",\n",
    "            \"FullReco_PrimaryVertex_Z\",\n",
    "        ],\n",
    "        \"topk\": 1,\n",
    "        \"count\": False,\n",
    "    },\n",
    "    \"met\": {\n",
    "        \"cols\": [\n",
    "            \"FullReco_PUPPIMET_MET\",\n",
    "            \"FullReco_PUPPIMET_Phi\",\n",
    "            \"FullReco_MET_MET\",\n",
    "            \"FullReco_MET_Phi\",\n",
    "        ],\n",
    "        \"topk\": None,  # special scalar\n",
    "        \"count\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# SPECIFY HERE WHICH PROCESSES TO USE FOR THE CLASSIFIER TRAINING\n",
    "# The keys are labels used in the training\n",
    "# The values are the exact process names as in the list below\n",
    "\n",
    "# These are the possible processes:\n",
    "\n",
    "# \"DY to ll\"\n",
    "# \"Z to vv + jet\"\n",
    "# \"Z to qq (uds)\"\n",
    "# \"Z to bb\"\n",
    "# \"Z to cc\"\n",
    "# \"W to lv\"\n",
    "# \"W to qq\"\n",
    "# \"gamma\"\n",
    "# \"gamma + V\"\n",
    "# \"tri-gamma\"\n",
    "#\n",
    "# \"QCD inclusive\"\n",
    "# \"QCD bb\"\n",
    "# \"Minbias / Soft QCD\"\n",
    "#\n",
    "# \"tt all-hadr\"\n",
    "# \"tt semi-lept\"\n",
    "# \"tt all-lept\"\n",
    "# \"ttH incl\"\n",
    "# \"tttt\"\n",
    "# \"ttW incl\"\n",
    "# \"ttZ incl\"\n",
    "#\n",
    "# \"WW all-leptonic\"\n",
    "# \"WW all-hadronic\"\n",
    "# \"WW semi-leptonic\"\n",
    "# \"WZ all-leptonic\"\n",
    "# \"WZ all-hadronic\"\n",
    "# \"WZ semi-leptonic\"\n",
    "# \"ZZ all-leptonic\"\n",
    "# \"ZZ all-hadronic\"\n",
    "# \"ZZ semi-leptonic\"\n",
    "# \"VVV\"\n",
    "# \"VH incl\"\n",
    "#\n",
    "# \"ggHbb\"\n",
    "# \"ggHcc\"\n",
    "# \"ggHgammagamma\"\n",
    "# \"ggHgluglu\"\n",
    "# \"ggHtautau\"\n",
    "# \"ggHWW\"\n",
    "# \"ggHZZ\"\n",
    "# \"VBFHbb\"\n",
    "# \"VBFHcc\"\n",
    "# \"VBFHgammagamma\"\n",
    "# \"VBFHgluglu\"\n",
    "# \"VBFHtautau\"\n",
    "# \"VBFHWW\"\n",
    "# \"VBFHZZ\"\n",
    "#\n",
    "# \"HH 4b\"\n",
    "# \"HH bbtautau\"\n",
    "# \"HH bbWW\"\n",
    "# \"HH bbZZ\"\n",
    "# \"HH bbgammagamma\"\n",
    "\n",
    "TO_CLASSIFY = {\n",
    "    \"QCD\": \"QCD inclusive\",\n",
    "    \"ggHbb\": \"ggHbb\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty name -> folder name (from your mapping)\n",
    "PROCESS_TO_FOLDER = {\n",
    "    # DY / Z / W\n",
    "    \"DY to ll\": \"DYJetsToLL_13TeV-madgraphMLM-pythia8\",\n",
    "    \"Z to vv + jet\": \"ZJetsTovv_13TeV-madgraphMLM-pythia8\",\n",
    "    \"Z to qq (uds)\": \"ZJetsToQQ_13TeV-madgraphMLM-pythia8\",\n",
    "    \"Z to bb\": \"ZJetsTobb_13TeV-madgraphMLM-pythia8\",\n",
    "    \"Z to cc\": \"ZJetsTocc_13TeV-madgraphMLM-pythia8\",\n",
    "    \"W to lv\": \"WJetsToLNu_13TeV-madgraphMLM-pythia8\",\n",
    "    \"W to qq\": \"WJetsToQQ_13TeV-madgraphMLM-pythia8\",\n",
    "    \"gamma\": \"gamma\",\n",
    "    \"gamma + V\": \"gamma_V\",\n",
    "    \"tri-gamma\": \"tri_gamma\",\n",
    "    # QCD\n",
    "    \"QCD inclusive\": \"QCD_HT50toInf\",\n",
    "    \"QCD bb\": \"QCD_HT50tobb\",\n",
    "    \"Minbias / Soft QCD\": \"minbias\",\n",
    "    # top\n",
    "    \"tt all-hadr\": \"tt0123j_5f_ckm_LO_MLM_hadronic\",\n",
    "    \"tt semi-lept\": \"tt0123j_5f_ckm_LO_MLM_semiLeptonic\",\n",
    "    \"tt all-lept\": \"tt0123j_5f_ckm_LO_MLM_leptonic\",\n",
    "    \"ttH incl\": \"ttH_incl\",\n",
    "    \"tttt\": \"tttt_incl\",\n",
    "    \"ttW incl\": \"ttW_incl\",\n",
    "    \"ttZ incl\": \"ttZ_incl\",\n",
    "    # dibosons\n",
    "    \"WW all-leptonic\": \"WW_leptonic\",\n",
    "    \"WW all-hadronic\": \"WW_hadronic\",\n",
    "    \"WW semi-leptonic\": \"WW_semileptonic\",\n",
    "    \"WZ all-leptonic\": \"WZ_leptonic\",\n",
    "    \"WZ all-hadronic\": \"WZ_hadronic\",\n",
    "    \"WZ semi-leptonic\": \"WZ_semileptonic\",\n",
    "    \"ZZ all-leptonic\": \"ZZ_leptonic\",\n",
    "    \"ZZ all-hadronic\": \"ZZ_hadronic\",\n",
    "    \"ZZ semi-leptonic\": \"ZZ_semileptonic\",\n",
    "    \"VVV\": \"VVV_incl\",\n",
    "    \"VH incl\": \"VH_incl\",\n",
    "    # single-Higgs\n",
    "    \"ggHbb\": \"ggHbb\",\n",
    "    \"ggHcc\": \"ggHcc\",\n",
    "    \"ggHgammagamma\": \"ggHgammagamma\",\n",
    "    \"ggHgluglu\": \"ggHgluglu\",\n",
    "    \"ggHtautau\": \"ggHtautau\",\n",
    "    \"ggHWW\": \"ggHWW\",\n",
    "    \"ggHZZ\": \"ggHZZ\",\n",
    "    \"VBFHbb\": \"VBFHbb\",\n",
    "    \"VBFHcc\": \"VBFHcc\",\n",
    "    \"VBFHgammagamma\": \"VBFHgammagamma\",\n",
    "    \"VBFHgluglu\": \"VBFHgluglu\",\n",
    "    \"VBFHtautau\": \"VBFHtautau\",\n",
    "    \"VBFHWW\": \"VBFHWW\",\n",
    "    \"VBFHZZ\": \"VBFHZZ\",\n",
    "    # di-Higgs\n",
    "    \"HH 4b\": \"HH_4b\",\n",
    "    \"HH bbtautau\": \"HH_bbtautau\",\n",
    "    \"HH bbWW\": \"HH_bbWW\",\n",
    "    \"HH bbZZ\": \"HH_bbZZ\",\n",
    "    \"HH bbgammagamma\": \"HH_bbgammagamma\",\n",
    "}\n",
    "\n",
    "CLASS_NAMES = list(TO_CLASSIFY.keys())\n",
    "PRETTY = {c: TO_CLASSIFY[c] for c in CLASS_NAMES}\n",
    "FOLDER = {c: PROCESS_TO_FOLDER[PRETTY[c]] for c in CLASS_NAMES}\n",
    "LABELS = {c: i for i, c in enumerate(CLASS_NAMES)}\n",
    "print(\"Classes:\", CLASS_NAMES)\n",
    "print(\"Pretty per class:\", PRETTY)\n",
    "print(\"Folder per class:\", FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c10a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_cols(config=DATASETS_CONFIG):\n",
    "    cols = []\n",
    "    for spec in config.values():\n",
    "        cols.extend(spec[\"cols\"])\n",
    "    return cols\n",
    "\n",
    "\n",
    "def compute_vlen(config=DATASETS_CONFIG):\n",
    "    total = 0\n",
    "    for spec in config.values():\n",
    "        ncols = len(spec[\"cols\"])\n",
    "        topk = spec[\"topk\"]\n",
    "        if topk is None:  # scalars\n",
    "            total += ncols\n",
    "        else:\n",
    "            total += ncols * topk\n",
    "        if spec.get(\"count\", False):\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "\n",
    "ALL_COLS = get_all_cols(DATASETS_CONFIG)\n",
    "VLEN = compute_vlen(DATASETS_CONFIG)\n",
    "print(\"The length of the feature vector is \" + str(VLEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pack_topk_batch(pt, *others, k: int, fill: float):\n",
    "    \"\"\"\n",
    "    Picks top k pt (or whatever is put in the first argument) particles/jets/vertices with pt and others as features.\n",
    "    This is vectorized: pt and others are ak.Arrays of shape (n_events, var).\n",
    "    Returns: np.ndarray of shape (n_events, k, n_features)\n",
    "    \"\"\"\n",
    "    # stack features → shape (n_events, n_objects, n_features)\n",
    "    arrays = (pt,) + others\n",
    "    expanded = [arr[:, :, None] for arr in arrays]\n",
    "    stacked = ak.concatenate(expanded, axis=2)\n",
    "\n",
    "    # argsort by pt descending (feature 0)\n",
    "    stacked = ak.values_astype(stacked, np.float32)\n",
    "    stacked = stacked[ak.argsort(stacked[:, :, 0], axis=1, ascending=False)]\n",
    "\n",
    "    # take top-k\n",
    "    topk = stacked[:, :k, :]\n",
    "    n_features = len(arrays)\n",
    "\n",
    "    # pad to length k (for events with <k objects) and fill with n_features*[fill]\n",
    "    topk = ak.pad_none(topk, k, axis=1)\n",
    "    fill_list = n_features * [fill]\n",
    "    topk = ak.fill_none(topk, fill_list, axis=1)\n",
    "\n",
    "    # transform to regular numpy array\n",
    "    topk_np = ak.to_numpy(topk)\n",
    "\n",
    "    return topk_np\n",
    "\n",
    "\n",
    "def _pack_leading_batch(pt, *others, fill: float):\n",
    "    \"\"\"\n",
    "    Picks top 1 pt (or whatever is put in the first argument) particles/jets/vertices with pt and others as features.\n",
    "    This is vectorized: pt and others are ak.Arrays of shape (n_events, var).\n",
    "    Returns: np.ndarray of shape (n_events, 1, n_features)\n",
    "    \"\"\"\n",
    "    # stack features → shape (n_events, n_objects, n_features)\n",
    "    arrays = (pt,) + others\n",
    "    expanded = [arr[:, :, None] for arr in arrays]\n",
    "    stacked = ak.concatenate(expanded, axis=2)\n",
    "\n",
    "    # argmax by max pt (feature 0)\n",
    "    stacked = ak.values_astype(stacked, np.float32)\n",
    "    leading = stacked[ak.argmax(stacked[:, :, 0], axis=1, keepdims=True)]\n",
    "    n_features = len(arrays)\n",
    "\n",
    "    # pad to length 1 (for events with <1 objects) and fill with n_features*[fill]\n",
    "    leading = ak.pad_none(leading, 1, axis=1)\n",
    "    fill_list = n_features * [fill]\n",
    "    leading = ak.fill_none(leading, fill_list, axis=1)\n",
    "\n",
    "    # transform to regular numpy array\n",
    "    leading_np = ak.to_numpy(leading)\n",
    "\n",
    "    return leading_np\n",
    "\n",
    "\n",
    "def build_vectors_batch(batch, config=DATASETS_CONFIG, fill=0.0):\n",
    "    \"\"\"\n",
    "    batch: dict mapping column name to the corresponding awkward array\n",
    "    config: DATASETS_CONFIG dict\n",
    "    fill: float fill value everywhere it needs to be padded\n",
    "\n",
    "    Returns: np.ndarray of shape (n_events, VLEN)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for name, spec in config.items():\n",
    "        cols = spec[\"cols\"]\n",
    "        topk = spec[\"topk\"]\n",
    "\n",
    "        if topk is None:\n",
    "            # --- scalars (e.g. MET) ---\n",
    "            vals = [ak.to_numpy(ak.fill_none(batch[c], fill)).reshape(-1, 1) for c in cols]\n",
    "            group = np.concatenate(vals, axis=1)  # (n_events, ncols)\n",
    "\n",
    "        elif topk == 1:\n",
    "            # --- leading object ---\n",
    "            arrays = [batch[c] for c in cols]\n",
    "            group = _pack_leading_batch(*arrays, fill=fill)\n",
    "            group = group.reshape(len(group), -1)\n",
    "\n",
    "        else:\n",
    "            # --- top-k objects ---\n",
    "            arrays = [batch[c] for c in cols]\n",
    "            group = _pack_topk_batch(*arrays, k=topk, fill=fill)\n",
    "            group = group.reshape(len(group), -1)\n",
    "\n",
    "        features.append(group)\n",
    "\n",
    "        # --- optional count feature ---\n",
    "        if spec.get(\"count\", False):\n",
    "            nobj = ak.num(batch[cols[0]], axis=1).to_numpy().reshape(-1, 1)\n",
    "            features.append(nobj)\n",
    "\n",
    "    # concatenate everything → final feature vector\n",
    "    out = np.concatenate(features, axis=1)\n",
    "    return out  # shape (n_events, VLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ebbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_map(config: dict, out_dir: str, vlen: int):\n",
    "    \"\"\"Save a single feature_map.json describing your flattened vector layout.\"\"\"\n",
    "    feature_map = {}\n",
    "    offset = 0\n",
    "\n",
    "    for group_name, cfg in config.items():\n",
    "        cols = cfg[\"cols\"]\n",
    "        topk = cfg[\"topk\"]\n",
    "        count = cfg.get(\"count\", False)\n",
    "\n",
    "        if topk is None:\n",
    "            size = len(cols)\n",
    "        else:\n",
    "            size = topk * len(cols)\n",
    "            if count:\n",
    "                size += 1\n",
    "\n",
    "        feature_map[group_name] = {\n",
    "            \"start\": int(offset),\n",
    "            \"end\": int(offset + size),\n",
    "            \"columns\": cols,\n",
    "            \"topk\": topk,\n",
    "            \"count\": count,\n",
    "        }\n",
    "        offset += size\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with open(os.path.join(out_dir, \"feature_map.json\"), \"w\") as f:\n",
    "        json.dump(feature_map, f, indent=2)\n",
    "    print(f\"✓ feature_map.json saved → {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EOSDataset(IterableDataset):\n",
    "    def __init__(self, base_dir, seed=42, per_class_limit=None, batch_size=512):\n",
    "        \"\"\"\n",
    "        base_dir: EOS root dir with all class folders\n",
    "        seed: random seed\n",
    "        per_class_limit: max number of events per class (optional)\n",
    "        batch_size: how many rows to pull from parquet at once\n",
    "        \"\"\"\n",
    "        self.base_dir = base_dir\n",
    "        self.seed = seed\n",
    "        self.per_class_limit = per_class_limit\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Step 1: collect parquet files for all classes\n",
    "        all_files = []\n",
    "        for cname in CLASS_NAMES:\n",
    "            folder = os.path.join(self.base_dir, FOLDER[cname])\n",
    "            files = [\n",
    "                os.path.join(folder, f)\n",
    "                for f in os.listdir(folder)\n",
    "                if f.endswith(\".parquet\") and not f.startswith(\".\")\n",
    "            ]\n",
    "            for f in files:\n",
    "                all_files.append((cname, f))\n",
    "\n",
    "        # Step 2: shuffle file order\n",
    "        rng = random.Random(self.seed)\n",
    "        rng.shuffle(all_files)\n",
    "\n",
    "        # Step 3: iterate over files\n",
    "        counters = {c: 0 for c in CLASS_NAMES}\n",
    "        for cname, path in all_files:\n",
    "            if self.per_class_limit and counters[cname] >= self.per_class_limit:\n",
    "                continue\n",
    "            with pq.ParquetFile(path) as pqf:\n",
    "                for batch in pqf.iter_batches(columns=ALL_COLS, batch_size=self.batch_size):\n",
    "                    # convert arrow batch → awkward\n",
    "                    tbl = pa.Table.from_batches([batch])\n",
    "                    arrays = {col: ak.from_arrow(tbl[col]) for col in ALL_COLS}\n",
    "                    # vectorize into np.ndarray (N, VLEN)\n",
    "                    feats = build_vectors_batch(arrays, DATASETS_CONFIG)\n",
    "                    n = feats.shape[0]\n",
    "\n",
    "                    # enforce per-class limit\n",
    "                    if self.per_class_limit:\n",
    "                        remain = self.per_class_limit - counters[cname]\n",
    "                        if remain <= 0:\n",
    "                            break\n",
    "                        feats = feats[:remain]\n",
    "                        n = feats.shape[0]\n",
    "\n",
    "                    counters[cname] += n\n",
    "                    labels = np.full((n,), LABELS[cname], dtype=np.int64)\n",
    "\n",
    "                    yield {\"x\": feats, \"y\": labels}\n",
    "\n",
    "                    if self.per_class_limit and counters[cname] >= self.per_class_limit:\n",
    "                        break\n",
    "\n",
    "\n",
    "class ShuffleBuffer(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, dataset, buffer_size=10000, seed=42):\n",
    "        \"\"\"\n",
    "        buffer_size: how many whole batches to buffer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.buffer_size = buffer_size\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "    def __iter__(self):\n",
    "        buf = []\n",
    "        for batch in self.dataset:\n",
    "            buf.append(batch)\n",
    "            if len(buf) >= self.buffer_size:\n",
    "                idx = self.rng.randrange(len(buf))\n",
    "                yield buf.pop(idx)\n",
    "        while buf:\n",
    "            idx = self.rng.randrange(len(buf))\n",
    "            yield buf.pop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_eos(local_dir, eos_dir):\n",
    "    os.makedirs(os.path.dirname(eos_dir), exist_ok=True)\n",
    "    print(f\"→ Moving {local_dir} → {eos_dir}\")\n",
    "    shutil.move(local_dir, eos_dir)\n",
    "    print(f\"✅ Moved to {eos_dir}\")\n",
    "\n",
    "\n",
    "def vectorized_to_local(\n",
    "    base_dir, config, out_dir, split_ratio=0.8, per_class_limit=None, batch_size=512\n",
    "):\n",
    "    \"\"\"Stream from EOS, vectorize, and save each Parquet shard as a .npy pair.\"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    save_feature_map(config, out_dir, VLEN)\n",
    "\n",
    "    for cname in CLASS_NAMES:\n",
    "        folder = os.path.join(base_dir, FOLDER[cname])\n",
    "        files = sorted(\n",
    "            f for f in os.listdir(folder) if f.endswith(\".parquet\") and not f.startswith(\".\")\n",
    "        )\n",
    "        counters = 0\n",
    "\n",
    "        for f in files:\n",
    "            if per_class_limit and counters >= per_class_limit:\n",
    "                break\n",
    "            path = os.path.join(folder, f)\n",
    "            print(f\"→ Processing {path}\")\n",
    "\n",
    "            all_feats, all_labels = [], []\n",
    "            with pq.ParquetFile(path) as pqf:\n",
    "                for batch in pqf.iter_batches(columns=ALL_COLS, batch_size=batch_size):\n",
    "                    tbl = pa.Table.from_batches([batch])\n",
    "                    arrays = {col: ak.from_arrow(tbl[col]) for col in ALL_COLS}\n",
    "                    feats = build_vectors_batch(arrays, config, fill=0.0)\n",
    "                    n = feats.shape[0]\n",
    "\n",
    "                    if per_class_limit:\n",
    "                        remain = per_class_limit - counters\n",
    "                        feats = feats[:remain]\n",
    "                        n = feats.shape[0]\n",
    "\n",
    "                    all_feats.append(feats)\n",
    "                    all_labels.append(np.full((n,), LABELS[cname], dtype=np.int64))\n",
    "                    counters += n\n",
    "\n",
    "                    if per_class_limit and counters >= per_class_limit:\n",
    "                        break\n",
    "\n",
    "            if not all_feats:\n",
    "                continue\n",
    "\n",
    "            feats_cat = np.concatenate(all_feats, axis=0)\n",
    "            labels_cat = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "            split = \"train\" if counters <= split_ratio * per_class_limit else \"val\"\n",
    "            temp_save_dir = os.path.join(\n",
    "                AFS_TMPDIR, SAVE_LABEL, \"vectorized\", split, FOLDER[cname]\n",
    "            )\n",
    "            save_dir = os.path.join(EOS_TARGET, SAVE_LABEL, \"vectorized\", split, FOLDER[cname])\n",
    "            os.makedirs(temp_save_dir, exist_ok=True)\n",
    "\n",
    "            base = os.path.splitext(f)[0]\n",
    "            local_x_file = os.path.join(temp_save_dir, f\"{base}_x.npy\")\n",
    "            local_y_file = os.path.join(temp_save_dir, f\"{base}_y.npy\")\n",
    "            np.save(local_x_file, feats_cat)\n",
    "            np.save(local_y_file, labels_cat)\n",
    "            shutil.move(local_x_file, os.path.join(save_dir, local_x_file))\n",
    "            shutil.move(local_y_file, os.path.join(save_dir, local_y_file))\n",
    "\n",
    "            print(f\"  ✓ Saved {base}: {feats_cat.shape}\")\n",
    "\n",
    "    print(f\"✅ Finished vectorizing → {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dae845",
   "metadata": {},
   "outputs": [],
   "source": [
    "AFS_VEC_OUT = os.path.join(AFS_TMPDIR, SAVE_LABEL, \"vectorized\")\n",
    "EOS_VEC_OUT = os.path.join(EOS_TARGET, SAVE_LABEL, \"vectorized\")\n",
    "AFS_PREPROC_OUT = os.path.join(AFS_TMPDIR, \"vectorized_preprocessed\")\n",
    "EOS_PREPROC_OUT = os.path.join(EOS_TARGET, \"vectorized_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stage 1: vectorize to local AFS scratch ---\n",
    "vectorized_to_local(\n",
    "    EOS_BASE,\n",
    "    DATASETS_CONFIG,\n",
    "    AFS_VEC_OUT,\n",
    "    split_ratio=TRAIN_VAL_RATIO,\n",
    "    per_class_limit=EVENTS_PER_CLASS,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "# --- Stage 2: move to EOS ---\n",
    "move_to_eos(AFS_VEC_OUT, EOS_VEC_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalVectorDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, base_dir, seed=42, per_class_limit=None):\n",
    "        super().__init__()\n",
    "        self.base_dir = base_dir  # path to vectorized/\n",
    "        self.seed = seed\n",
    "        self.per_class_limit = per_class_limit\n",
    "\n",
    "    def __iter__(self):\n",
    "        class_dirs = [\n",
    "            os.path.join(self.base_dir, folder)\n",
    "            for folder in os.listdir(self.base_dir)\n",
    "            if os.path.isdir(os.path.join(self.base_dir, folder))\n",
    "        ]\n",
    "\n",
    "        rng = random.Random(self.seed)\n",
    "        rng.shuffle(class_dirs)\n",
    "\n",
    "        counters = {}\n",
    "        for class_dir in class_dirs:\n",
    "            cname = os.path.basename(class_dir)\n",
    "            files_x = sorted(f for f in os.listdir(class_dir) if f.endswith(\"_x.npy\"))\n",
    "            rng.shuffle(files_x)\n",
    "            counters[cname] = 0\n",
    "\n",
    "            for fx in files_x:\n",
    "                fy = fx.replace(\"_x.npy\", \"_y.npy\")\n",
    "                X = np.load(os.path.join(class_dir, fx))\n",
    "                y = np.load(os.path.join(class_dir, fy))\n",
    "\n",
    "                if self.per_class_limit:\n",
    "                    remain = self.per_class_limit - counters[cname]\n",
    "                    if remain <= 0:\n",
    "                        break\n",
    "                    X = X[:remain]\n",
    "                    y = y[:remain]\n",
    "                    counters[cname] += len(y)\n",
    "                else:\n",
    "                    counters[cname] += len(y)\n",
    "\n",
    "                yield {\"x\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab083d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_stream = LocalVectorDataset(\n",
    "    os.path.join(EOS_TARGET, SAVE_LABEL, \"vectorized\", \"val\"), seed=SEED + 1000\n",
    ")\n",
    "\n",
    "Xv, Yv = [], []\n",
    "for batch in val_stream:\n",
    "    Xv.append(batch[\"x\"])\n",
    "    Yv.append(batch[\"y\"])\n",
    "\n",
    "X_val = torch.tensor(np.concatenate(Xv, axis=0), dtype=torch.float32)\n",
    "y_val = torch.tensor(np.concatenate(Yv, axis=0), dtype=torch.long)\n",
    "print(\"Val set:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b79481",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = [0, 2, 3, 5, 140, 141, 144, 145, 166, 178, 179]\n",
    "legends = [\n",
    "    \"lead_particle_pt\",\n",
    "    \"lead_particle_phi\",\n",
    "    \"lead_particle_charge\",\n",
    "    \"lead_particle_pid\",\n",
    "    \"tot_particle_count\",\n",
    "    \"lead_jet_pt\",\n",
    "    \"lead_jet_mass\",\n",
    "    \"lead_jet_btag\",\n",
    "    \"lead_electron_pt\",\n",
    "    \"lead_vertex_sumpt2\",\n",
    "    \"lead_vertex_z\",\n",
    "]\n",
    "for i, legend in zip(to_plot, legends):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(X_val[:, i], bins=50, label=legend)\n",
    "    plt.xlabel(\"Value per event\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae5062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mean_std(vectorized_dir, per_class_limit=None):\n",
    "    count = 0\n",
    "    mean = None\n",
    "    M2 = None\n",
    "\n",
    "    ds = LocalVectorDataset(vectorized_dir, seed=SEED + 222, per_class_limit=per_class_limit)\n",
    "\n",
    "    for batch in ds:\n",
    "        xb = torch.tensor(batch[\"x\"], dtype=torch.float32)\n",
    "        for x in xb:\n",
    "            if mean is None:\n",
    "                mean = torch.zeros_like(x)\n",
    "                M2 = torch.zeros_like(x)\n",
    "            count += 1\n",
    "            delta = x - mean\n",
    "            mean += delta / count\n",
    "            delta2 = x - mean\n",
    "            M2 += delta * delta2\n",
    "\n",
    "    var = M2 / max(count - 1, 1)\n",
    "    std = torch.sqrt(var + 1e-6)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "MEAN, STD = estimate_mean_std(\n",
    "    os.path.join(EOS_TARGET, SAVE_LABEL, \"vectorized\", \"train\"), per_class_limit=10000\n",
    ")\n",
    "print(\"Mean:\", MEAN[:10])\n",
    "print(\"Std:\", STD[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c04567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize validation set once\n",
    "Xv, Yv = [], []\n",
    "for batch in val_stream:\n",
    "    xb = torch.tensor(batch[\"x\"], dtype=torch.float32)\n",
    "    yb = torch.tensor(batch[\"y\"], dtype=torch.long)\n",
    "    # normalize here\n",
    "    xb = (xb - MEAN) / STD\n",
    "    Xv.append(xb)\n",
    "    Yv.append(yb)\n",
    "\n",
    "# concatenate along batch dimension\n",
    "X_val = torch.cat(Xv, dim=0)\n",
    "y_val = torch.cat(Yv, dim=0)\n",
    "\n",
    "print(\"Val set:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96790778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, num_classes, d=VLEN, h=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, h),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h, h // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h // 2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "model = TinyMLP(num_classes=len(CLASS_NAMES), d=VLEN, h=256).to(DEVICE)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158db865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y, batch=512):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X), batch):\n",
    "            xb = X[i : i + batch].to(DEVICE)\n",
    "            yb = y[i : i + batch].to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += int((pred == yb).sum().item())\n",
    "            total += int(len(yb))\n",
    "    model.train()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Starting epoch {epoch}\")\n",
    "\n",
    "    train_stream = LocalVectorDataset(\n",
    "        os.path.join(EOS_TARGET, SAVE_LABEL, \"vectorized\", \"train\"),\n",
    "        seed=SEED + epoch,\n",
    "    )\n",
    "\n",
    "    shuffled_train = ShuffleBuffer(train_stream, buffer_size=10000, seed=SEED + epoch)\n",
    "    train_loader = DataLoader(shuffled_train, batch_size=None, num_workers=0)\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        x = batch[\"x\"].to(dtype=torch.float32, device=DEVICE)\n",
    "        y = batch[\"y\"].to(dtype=torch.long, device=DEVICE)\n",
    "\n",
    "        x = (x - MEAN) / STD\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss {running_loss / 20:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    acc = evaluate(model, X_val, y_val)\n",
    "    print(f\"[Epoch {epoch}] Validation accuracy: {acc * 100:.2f}% | Classes: {CLASS_NAMES}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5237a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4bb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "fm_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
