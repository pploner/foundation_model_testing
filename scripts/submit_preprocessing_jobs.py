#!/usr/bin/env python3
"""
submit_preprocessing_jobs.py

Use this script to batch submit preprocessing jobs to Condor.

Reads Hydra configs, creates stats file, splits it into chunks of ‚â§ MAX_FILES_PER_JOB,
and directly submits each chunk to Condor to run `PreprocessingPipeline` inside the Apptainer container.
"""

import rootutils
rootutils.setup_root(__file__, indicator=".project-root", pythonpath=True)
import json
import subprocess
import hydra
from math import ceil
from pathlib import Path
from omegaconf import DictConfig, OmegaConf

from src.preprocessing.preprocess import PreprocessingPipeline

# -----------------------------------------------------------------------------
# CONFIG
# -----------------------------------------------------------------------------
MAX_FILES_PER_JOB = 50
JOB_FLAVOUR = "tomorrow"  # ~24h jobs


PROJECT_DIR = Path(__file__).resolve().parents[1]
WRAPPER_SCRIPT = PROJECT_DIR / "src/preprocessing/wrapper_preprocess.sh"
LOG_DIR = PROJECT_DIR / "logs/condor_logs/preprocessing"

# -----------------------------------------------------------------------------
# MAIN
# -----------------------------------------------------------------------------
@hydra.main(
    config_path="../configs",
    config_name="vectorize_preprocess.yaml",
    version_base="1.3",
)
def main(cfg: DictConfig):

    print("üü¢ Hydra config composed successfully")
    # -----------------------------------------------------------------------------
    # LOAD CONFIGS
    # -----------------------------------------------------------------------------
    print("üü¢ Loading Hydra configs...")

    paths_cfg = cfg.paths
    data_cfg = cfg.data
    pre_cfg = cfg.preprocess
    class_order = data_cfg.to_classify              # {"QCD": "QCD inclusive", ...}
    process_to_folder = data_cfg.process_to_folder     # {"QCD inclusive": "QCD_HT50toInf", ...}

    LOG_DIR.mkdir(parents=True, exist_ok=True)

    # -----------------------------------------------------------------------------
    # Build PreprocessingPipeline (fit_only)
    # -----------------------------------------------------------------------------
    print("üü° Preparing preprocessing pipeline (fit_only)...")

    paths = {
        "eos_vec_dir": data_cfg.paths.eos_vec_dir,
        "tmp_preproc_dir": data_cfg.paths.tmp_preproc_dir,
        "eos_preproc_dir": data_cfg.paths.eos_preproc_dir,
    }

    stats_path = Path(paths["eos_preproc_dir"]) / "norm_stats.json"

    if not stats_path.exists():
        print("‚ÑπÔ∏è norm_stats.json not found, will fit normalization stats...")
        # Force mode = fit_only without touching config files
        pre_cfg_local = OmegaConf.to_container(pre_cfg, resolve=True)
        pre_cfg_local["mode"] = "fit_only"
        pre_cfg_local["enabled"] = True
        pre_cfg_local = OmegaConf.create(pre_cfg_local)

        pipeline = PreprocessingPipeline(
            paths=paths,
            preprocess_cfg=pre_cfg_local,
            process_to_folder=process_to_folder,
            class_order=class_order,
            device="cpu",
        )

        pipeline.run()
    else:
        print("‚ÑπÔ∏è norm_stats.json found, skipping fitting step.")

    if not stats_path.exists():
        raise RuntimeError(f"‚ùå norm_stats.json missing after fit_only: {stats_path}")

    print(f"‚úÖ Fitted normalization stats ‚Üí {stats_path}")

    # -----------------------------------------------------------------------------
    # SCAN VECTORIZED FILES
    # -----------------------------------------------------------------------------
    print("\nüü¢ Scanning vectorized files...")

    entries = []  # list of (folder, split, filename)

    for split in ("train", "val", "test"):
        for cls in class_order:
            cls_folder = process_to_folder[cls]
            vec_dir = Path(paths["eos_vec_dir"]) / split / cls_folder
            pre_dir = Path(paths["eos_preproc_dir"]) / split / cls_folder
            npy_files = list(vec_dir.glob("*_x.npy"))
            target_npy_files = list(pre_dir.glob("*_x.npy"))
            target_npy_file_names = {f.name for f in target_npy_files}
            for f in npy_files:
                if f.name not in target_npy_file_names:
                    entries.append((cls_folder, split, f.name))

    num_files = len(entries)
    n_jobs = ceil(num_files / MAX_FILES_PER_JOB)

    print(f"üü° Found {num_files} vectorized files ‚Üí {n_jobs} jobs (‚â§{MAX_FILES_PER_JOB}/job)")

    # -----------------------------------------------------------------------------
    # SUBMIT ALL JOBS
    # -----------------------------------------------------------------------------
    for i in range(n_jobs):
        start = i * MAX_FILES_PER_JOB
        end = start + MAX_FILES_PER_JOB
        chunk = entries[start:end]
        submit_job(i, chunk)

    print("\n‚úÖ All preprocessing jobs submitted.")


# -----------------------------------------------------------------------------
# FUNCTION TO SUBMIT A SINGLE JOB
# -----------------------------------------------------------------------------
def submit_job(job_idx, chunk):
    # Build submanifest structure
    submanifest = {}
    for folder, split, fname in chunk:
        if folder not in submanifest:
            submanifest[folder] = {"train": [], "val": [], "test": []}
        submanifest[folder][split].append(fname)

    # Save submanifest
    manifest_path = LOG_DIR / f"manifest_{job_idx:04d}.json"
    with open(manifest_path, "w") as f:
        json.dump(submanifest, f, indent=2)

    log_out = LOG_DIR / f"job_{job_idx:04d}.out"
    log_err = LOG_DIR / f"job_{job_idx:04d}.err"
    log_log = LOG_DIR / f"job_{job_idx:04d}.log"

    submit_content = f"""\
executable = {WRAPPER_SCRIPT}
arguments  = {manifest_path} {PROJECT_DIR}
initialdir = {LOG_DIR}

output = {log_out}
error  = {log_err}
log    = {log_log}

stream_output = True
stream_error = True

run_as_owner = True
+JobFlavour = "{JOB_FLAVOUR}"
getenv = True
request_cpus = 4
environment = "WRAPPER_DEBUG=1; OMP_NUM_THREADS=4; MKL_NUM_THREADS=4"

queue
"""

    sub_path = LOG_DIR / f"preprocess_job_{job_idx:04d}.sub"
    with open(sub_path, "w") as f:
        f.write(submit_content)

    subprocess.run(["condor_submit", str(sub_path)], check=False)
    print(f"üöÄ Submitted job {job_idx:04d} ({len(chunk)} files)")

if __name__ == "__main__":
    main()
